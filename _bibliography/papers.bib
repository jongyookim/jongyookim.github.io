@article{gaoMPSNeRFGeneralizable3D2022a,
  title = {{{MPS-NeRF}}: {{Generalizable 3D Human Rendering From Multiview Images}}},
  shorttitle = {{{MPS-NeRF}}},
  author = {Gao, Xiangjun and Yang, Jiaolong and Kim, Jongyoo and Peng, Sida and Liu, Zicheng and Tong, Xin},
  year = {2022},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1--12},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2022.3205910},
  urldate = {2024-05-04},
  abstract = {There has been rapid progress recently on 3D human rendering, including novel view synthesis and pose animation, based on the advances of neural radiance fields (NeRF). However, most existing methods focus on person-specific training and their training typically requires multi-view videos. This paper deals with a new challenging task -- rendering novel views and novel poses for a person unseen in training, using only multiview still images as input without videos. For this task, we propose a simple yet surprisingly effective method to train a generalizable NeRF with multiview images as conditional input. The key ingredient is a dedicated representation combining a canonical NeRF and a volume deformation scheme. Using a canonical space enables our method to learn shared properties of human and easily generalize to different people. Volume deformation is used to connect the canonical space with input and target images and query image features for radiance and density prediction. We leverage the parametric 3D human model fitted on the input images to derive the deformation, which works quite well in practice when combined with our canonical NeRF. The experiments on both real and synthetic data with the novel view synthesis and pose animation tasks collectively demonstrate the efficacy of our method.},
  file = {/Users/jongyoo/Zotero/storage/6CBWYCKN/Gao et al. - 2022 - MPS-NeRF Generalizable 3D Human Rendering From Mu.pdf}
}

@inproceedings{huangADNetLeveragingErrorBias2021,
  title = {{{ADNet}}: {{Leveraging Error-Bias Towards Normal Direction}} in {{Face Alignment}}},
  shorttitle = {{{ADNet}}},
  booktitle = {{{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Huang, Yangyu and Yang, Hao and Li, Chong and Kim, Jongyoo and Wei, Fangyun},
  year = {2021},
  month = oct,
  pages = {3060--3070},
  issn = {2380-7504},
  doi = {10.1109/ICCV48922.2021.00307},
  urldate = {2024-05-04},
  abstract = {The recent progress of CNN has dramatically improved face alignment performance. However, few works have paid attention to the error-bias with respect to error distribution of facial landmarks. In this paper, we investigate the error-bias issue in face alignment, where the distributions of landmark errors tend to spread along the tangent line to landmark curves. This error-bias is not trivial since it is closely connected to the ambiguous landmark labeling task. Inspired by this observation, we seek a way to leverage the error-bias property for better convergence of CNN model. To this end, we propose anisotropic direction loss (ADL) and anisotropic attention module (AAM) for coordinate and heatmap regression, respectively. ADL imposes strong binding force in normal direction for each landmark point on facial boundaries. On the other hand, AAM is an attention module which can get anisotropic attention mask focusing on the region of point and its local edge connected by adjacent points, it has a stronger response in tangent than in normal, which means relaxed constraints in the tangent. These two methods work in a complementary manner to learn both facial structures and texture details. Finally, we integrate them into an optimized end-to-end training pipeline named ADNet. Our ADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which demonstrates the effectiveness and robustness.},
  file = {/Users/jongyoo/Zotero/storage/ZD2RGETS/Huang et al. - 2021 - ADNet Leveraging Error-Bias Towards Normal Direct.pdf}
}

@inproceedings{huangFreeEnricherEnrichingFace2023,
  title = {{{FreeEnricher}}: Enriching Face Landmarks without Additional Cost},
  booktitle = {{{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Huang, Yangyu and Chen, Xi and Kim, Jongyoo and Yang, Hao and Li, Chong and Yang, Jiaolong and Chen, Dong},
  year = {2023},
  volume = {37},
  pages = {962--970},
  abstract = {Recent years have witnessed significant growth of face alignment. Though dense facial landmark is highly demanded in various scenarios, e.g., cosmetic medicine and facial beautification, most works only consider sparse face alignment. To address this problem, we present a framework that can enrich landmark density by existing sparse landmark datasets, e.g., 300W with 68 points and WFLW with 98 points. Firstly, we observe that the local patches along each semantic contour are highly similar in appearance. Then, we propose a weakly-supervised idea of learning the refinement ability on original sparse landmarks and adapting this ability to enriched dense landmarks. Meanwhile, several operators are devised and organized together to implement the idea. Finally, the trained model is applied as a plug-and-play module to the existing face alignment networks. To evaluate our method, we manually label the dense landmarks on 300W testset. Our method yields state-of-the-art accuracy not only in newly-constructed dense 300W testset but also in the original sparse 300W and WFLW testsets without additional cost.},
  copyright = {All rights reserved},
  file = {/Users/jongyoo/Zotero/storage/2IEVZ3ST/Huang et al. - 2023 - FreeEnricher enriching face landmarks without add.pdf}
}

@article{kimBlindSharpnessPrediction2016,
  title = {Blind Sharpness Prediction for Ultrahigh-Definition Video Based on Human Visual Resolution},
  author = {Kim, Haksub and Kim, Jongyoo and Oh, Taegeun and Lee, Sanghoon},
  year = {2016},
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {27},
  number = {5},
  pages = {951--964},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@inproceedings{kimDeepBlindImage2017,
  title = {Deep Blind Image Quality Assessment by Employing {{FR-IQA}}},
  booktitle = {{{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Kim, Jongyoo and Lee, Sanghoon},
  year = {2017},
  pages = {3180--3184},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@inproceedings{kimDeepBlindImage2018,
  title = {Deep Blind Image Quality Assessment by Learning Sensitivity Map},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kim, Jongyoo and Kim, Woojae and Lee, Sanghoon},
  year = {2018},
  pages = {6727--6731},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@article{kimDeepCNNBasedBlind2019,
  title = {Deep {{CNN-Based Blind Image Quality Predictor}}},
  author = {Kim, Jongyoo and Nguyen, Anh-Duc and Lee, Sanghoon},
  year = {2019},
  month = jan,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {30},
  number = {1},
  pages = {11--24},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2018.2829819},
  urldate = {2024-05-04},
  abstract = {Image recognition based on convolutional neural networks (CNNs) has recently been shown to deliver the state-of-the-art performance in various areas of computer vision and image processing. Nevertheless, applying a deep CNN to no-reference image quality assessment (NR-IQA) remains a challenging task due to critical obstacles, i.e., the lack of a training database. In this paper, we propose a CNN-based NR-IQA framework that can effectively solve this problem. The proposed method---deep image quality assessor (DIQA)---separates the training of NR-IQA into two stages: 1) an objective distortion part and 2) a human visual system-related part. In the first stage, the CNN learns to predict the objective error map, and then the model learns to predict subjective score in the second stage. To complement the inaccuracy of the objective error map prediction on the homogeneous region, we also propose a reliability map. Two simple handcrafted features were additionally employed to further enhance the accuracy. In addition, we propose a way to visualize perceptual error maps to analyze what was learned by the deep CNN model. In the experiments, the DIQA yielded the state-of-the-art accuracy on the various databases.},
  file = {/Users/jongyoo/Zotero/storage/H7YSEAN3/Kim et al. - 2019 - Deep CNN-Based Blind Image Quality Predictor.pdf}
}

@article{kimDeepConvolutionalNeural2017a,
  title = {Deep {{Convolutional Neural Models}} for {{Picture-Quality Prediction}}: {{Challenges}} and {{Solutions}} to {{Data-Driven Image Quality Assessment}}},
  shorttitle = {Deep {{Convolutional Neural Models}} for {{Picture-Quality Prediction}}},
  author = {Kim, Jongyoo and Zeng, Hui and Ghadiyaram, Deepti and Lee, Sanghoon and Zhang, Lei and Bovik, Alan C.},
  year = {2017},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {6},
  pages = {130--141},
  issn = {1558-0792},
  doi = {10.1109/MSP.2017.2736018},
  urldate = {2024-05-04},
  abstract = {Convolutional neural networks (CNNs) have been shown to deliver standout performance on a wide variety of visual information processing applications. However, this rapidly developing technology has only recently been applied with systematic energy to the problem of picture-quality prediction, primarily because of limitations imposed by a lack of adequate ground-truth human subjective data. This situation has begun to change with the development of promising data-gathering methods that are driving new approaches to deep-learning-based perceptual picture-quality prediction. Here, we assay progress in this rapidly evolving field, focusing, in particular, on new ways to collect large quantities of ground-truth data and on recent CNN-based picture-quality prediction models that deliver excellent results in a large, real-world, picture-quality database.},
  file = {/Users/jongyoo/Zotero/storage/J6CVZCZR/Kim et al. - 2017 - Deep Convolutional Neural Models for Picture-Quali.pdf}
}

@inproceedings{kimDeepLearningHuman2017,
  title = {Deep Learning of Human Visual Sensitivity in Image Quality Assessment Framework},
  booktitle = {{{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kim, Jongyoo and Lee, Sanghoon},
  year = {2017},
  pages = {1676--1684},
  copyright = {All rights reserved}
}

@inproceedings{kimDeepVideoQuality2018,
  title = {Deep {{Video Quality Assessor}}: {{From Spatio-Temporal Visual Sensitivity}} to a {{Convolutional Neural Aggregation Network}}},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {Kim, Woojae and Kim, Jongyoo and Ahn, Sewoong and Kim, Jinwoo and Lee, Sanghoon},
  year = {2018},
  pages = {219--234},
  copyright = {All rights reserved}
}

@article{kimDiverseAdjustableVersatile2021,
  title = {Diverse and {{Adjustable Versatile Image Enhancer}}},
  author = {Kim, Woojae and Nguyen, Anh-Duc and Kim, Jinwoo and Kim, Jongyoo and Oh, Heeseok and Lee, Sanghoon},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {80883--80896},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@article{kimFullyDeepBlind2017,
  title = {Fully {{Deep Blind Image Quality Predictor}}},
  author = {Kim, Jongyoo and Lee, Sanghoon},
  year = {2017},
  month = feb,
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  volume = {11},
  number = {1},
  pages = {206--220},
  issn = {1941-0484},
  doi = {10.1109/JSTSP.2016.2639328},
  urldate = {2024-05-04},
  abstract = {In general, owing to the benefits obtained from original information, full-reference image quality assessment (FR-IQA) achieves relatively higher prediction accuracy than no-reference image quality assessment (NR-IQA). By fully utilizing reference images, conventional FR-IQA methods have been investigated to produce objective scores that are close to subjective scores. In contrast, NR-IQA does not consider reference images; thus, its performance is inferior to that of FR-IQA. To alleviate this accuracy discrepancy between FR-IQA and NR-IQA methods, we propose a blind image evaluator based on a convolutional neural network (BIECON). To imitate FR-IQA behavior, we adopt the strong representation power of a deep convolutional neural network to generate a local quality map, similar to FR-IQA. To obtain the best results from the deep neural network, replacing hand-crafted features with automatically learned features is necessary. To apply the deep model to the NR-IQA framework, three critical problems must be resolved: 1) lack of training data; 2) absence of local ground truth targets; and 3) different purposes of feature learning. BIECON follows the FR-IQA behavior using the local quality maps as intermediate targets for conventional neural networks, which leads to NR-IQA prediction accuracy that is comparable with that of state-of-the-art FR-IQA methods.},
  file = {/Users/jongyoo/Zotero/storage/4NT6R2AL/Kim and Lee - 2017 - Fully Deep Blind Image Quality Predictor.pdf}
}

@article{kimImplementationOmnidirectionalHuman2015,
  title = {Implementation of an Omnidirectional Human Motion Capture System Using Multiple Kinect Sensors},
  author = {Kim, Junghwan and Lee, Inwoong and Kim, Jongyoo and Lee, Sanghoon},
  year = {2015},
  journal = {IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences},
  volume = {98},
  number = {9},
  pages = {2004--2008},
  publisher = {{The Institute of Electronics, Information and Communication Engineers}},
  copyright = {All rights reserved}
}

@inproceedings{kimLearningHighFidelityFace2021a,
  title = {Learning {{High-Fidelity Face Texture Completion}} without {{Complete Face Texture}}},
  booktitle = {{{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Kim, Jongyoo and Yang, Jiaolong and Tong, Xin},
  year = {2021},
  month = oct,
  pages = {13970--13979},
  issn = {2380-7504},
  doi = {10.1109/ICCV48922.2021.01373},
  urldate = {2024-05-04},
  abstract = {For face texture completion, previous methods typically use some complete textures captured by multiview imaging systems or 3D scanners for supervised learning. This paper deals with a new challenging problem - learning to complete invisible texture in a single face image without using any complete texture. We simply leverage a large corpus of face images of different subjects (e. g., FFHQ) to train a texture completion model in an unsupervised manner. To achieve this, we propose DSD-GAN, a novel deep neural network based method that applies two discriminators in UV map space and image space. These two discriminators work in a complementary manner to learn both facial structures and texture details. We show that their combination is essential to obtain high-fidelity results. Despite the network never sees any complete facial appearance, it is able to generate compelling full textures from single images.},
  file = {/Users/jongyoo/Zotero/storage/UHRNMRWE/Kim et al. - 2021 - Learning High-Fidelity Face Texture Completion wit.pdf}
}

@article{kimMNETMusicDrivenPluralistic2023,
  title = {{{MNET}}++: {{Music-Driven Pluralistic Dancing Toward Multiple Dance Genre Synthesis}}},
  shorttitle = {{{MNET}}++},
  author = {Kim, Jinwoo and Kwon, Beom and Kim, Jongyoo and Lee, Sanghoon},
  year = {2023},
  month = dec,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {12},
  pages = {15036--15050},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2023.3312092},
  urldate = {2024-05-01},
  abstract = {Numerous task-specific variants of autoregressive networks have been developed for dance generation. Nonetheless, a severe limitation remains in that all existing algorithms can return repeated patterns for a given initial pose, which may be inferior. We examine and analyze several key challenges of previous works, and propose variations in both model architecture (namely MNET++) and training methods to address these. In particular, we devise the beat synchronizer and dance synthesizer. First, generated dance should be locally and globally consistent with given music beats, circumvent repetitive patterns, and look realistic. To achieve this, the beat synchronizer implicitly catches the rhythm enabling it to stay in sync with the music as it dances. Then, the dance synthesizer infers the dance motions in a seamless patch-by-patch manner conditioned by music. Second, to generate diverse dance lines, adversarial learning is performed by leveraging the transformer architecture. Furthermore, MNET++ learns a dance genre-aware latent representation that is scalable for multiple domains to provide fine-grained user control according to the dance genre. Compared with the state-of-the-art methods, our method synthesizes plausible and diverse outputs according to multiple dance genres as well as generates remarkable dance sequences qualitatively and quantitatively.},
  file = {/Users/jongyoo/Zotero/storage/ECED9C2L/Kim et al. - 2023 - MNET++ Music-Driven Pluralistic Dancing Toward Mu.pdf}
}

@inproceedings{kimMultipleLevelFeaturebased2018,
  title = {Multiple Level Feature-Based Universal Blind Image Quality Assessment Model},
  booktitle = {{{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Kim, Jongyoo and Nguyen, Anh-Duc and Ahn, Sewoong and Luo, Chong and Lee, Sanghoon},
  year = {2018},
  pages = {291--295},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@inproceedings{kimNoreferencePerceptualSharpness2016,
  title = {No-Reference Perceptual Sharpness Assessment for Ultra-High-Definition Images},
  booktitle = {{{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Kim, Woojae and Kim, Haksub and Oh, Heeseok and Kim, Jongyoo and Lee, Sanghoon},
  year = {2016},
  pages = {86--90},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@article{kimPerceptualCrosstalkPrediction2016,
  title = {Perceptual Crosstalk Prediction on Autostereoscopic {{3D}} Display},
  author = {Kim, Taewan and Kim, Jongyoo and Kim, SeongYong and Cho, Sungho and Lee, Sanghoon},
  year = {2016},
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {27},
  number = {7},
  pages = {1450--1463},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@inproceedings{kimQualityAssessmentPerceptual2014,
  title = {Quality Assessment of Perceptual Crosstalk in Autostereoscopic Display},
  booktitle = {{{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Kim, Jongyoo and Kim, Taewan and Lee, Sanghoon},
  year = {2014},
  pages = {3484--3487},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@article{kimQualityAssessmentPerceptual2017,
  title = {Quality Assessment of Perceptual Crosstalk on Two-View Auto-Stereoscopic Displays},
  author = {Kim, Jongyoo and Kim, Taewan and Lee, Sanghoon and Bovik, Alan Conrad},
  year = {2017},
  journal = {IEEE Transactions on Image Processing},
  volume = {26},
  number = {10},
  pages = {4885--4899},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@inproceedings{kimVideoSharpnessPrediction2015,
  title = {Video Sharpness Prediction Based on Motion Blur Analysis},
  booktitle = {{{IEEE International Conference}} on {{Multimedia}} and {{Expo}} ({{ICME}})},
  author = {Kim, Jongyoo and Kim, Junghwan and Kim, Woojae and Lee, Jisoo and Lee, Sanghoon},
  year = {2015},
  pages = {1--6},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@article{kwonFrameworkImplementationImagebased2016,
  title = {Framework Implementation of Image-Based Indoor Localization System Using Parallel Distributed Computing},
  author = {Kwon, Beom and Jeon, Donghyun and Kim, Jongyoo and Kim, Junghwan and Kim, Doyoung and Song, Hyewon and Lee, Sanghoon},
  year = {2016},
  journal = {The Journal of Korean Institute of Communications and Information Sciences},
  volume = {41},
  number = {11},
  pages = {1490--1501},
  publisher = {{The Korean Institute of Commucations and Information Sciences}},
  copyright = {All rights reserved}
}

@inproceedings{kwonImplementationHumanAction2015,
  title = {Implementation of Human Action Recognition System Using Multiple {{Kinect}} Sensors},
  booktitle = {Advances in Multimedia Information {{Processing}}--{{PCM}} 2015},
  author = {Kwon, Beom and Kim, Doyoung and Kim, Junghwan and Lee, Inwoong and Kim, Jongyoo and Oh, Heeseok and Kim, Haksub and Lee, Sanghoon},
  year = {2015},
  pages = {334--343},
  publisher = {Springer International Publishing},
  copyright = {All rights reserved}
}

@article{leeDoubleReverseDiffusion2024,
  title = {Double Reverse Diffusion for Realistic Garment Reconstruction from Images},
  author = {Lee, Jeonghaeng and Nguyen, Duc and Kim, Jongyoo and Kang, Jiwoo and Lee, Sanghoon},
  year = {2024},
  month = jan,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {127},
  pages = {107404},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2023.107404},
  urldate = {2024-05-01},
  abstract = {Creating realistic digital 3D avatars has been getting more attention thanks to the introduction of new multimedia formats such as augmented and virtual reality. An important factor making avatars realistic is clothes. In this paper, we investigate a new method to reconstruct realistic garments from a set of images and body information. Early methods working on realistic images struggle to faithfully reconstruct the garment details. As deep learning is increasingly applied to geometric data which can conveniently represent garments, we devise a novel deep learning-based solution to the garment reconstruction problem. We offer a new perspective on the reconstruction problem and treat it as a reversion of the smoothing diffusion process. To achieve this goal, we propose to deform the smoothed human mesh into a clothed human via a Double Reverse Diffusion (DReD) process. For the first reverse diffusion, we introduce a novel operator called Graph Long Short-Term Memory (GraphLSTM) which recursively diffuses features to produce a deformed mesh by modeling the relationships between vertices. Then, the output mesh can be repeatedly upsampled and deformed by the above pipeline to obtain finer garment details, which can be seen as another reverse diffusion process. To obtain features for the reverse diffusion, we extract pixel-aligned features transferred from images and explore to incorporate the visibility of garments from the image viewpoints. Through detailed experiments on two public datasets, we demonstrate that DReD synthesizes more realistic wrinkled garments with lower errors and offers faster inference than previous methods.}
}

@article{leeIdentificationFrameworkPrintscan2017,
  title = {An Identification Framework for Print-Scan Books in a Large Database},
  author = {Lee, Sang-Hoon and Kim, Jongyoo and Lee, Sanghoon},
  year = {2017},
  journal = {Information sciences},
  volume = {396},
  pages = {33--54},
  publisher = {Elsevier},
  copyright = {All rights reserved}
}

@article{nguyenDeepVisualSaliency2018,
  title = {Deep Visual Saliency on Stereoscopic Images},
  author = {Nguyen, Anh-Duc and Kim, Jongyoo and Oh, Heeseok and Kim, Haksub and Lin, Weisi and Lee, Sanghoon},
  year = {2018},
  journal = {IEEE Transactions on Image Processing},
  volume = {28},
  number = {4},
  pages = {1939--1953},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@article{nguyenSingleImage3DReconstruction2024,
  title = {Single-{{Image}} 3-{{D Reconstruction}}: {{Rethinking Point Cloud Deformation}}},
  shorttitle = {Single-{{Image}} 3-{{D Reconstruction}}},
  author = {Nguyen, Anh-Duc and Choi, Seonghwa and Kim, Woojae and Kim, Jongyoo and Oh, Heeseok and Kang, Jiwoo and Lee, Sanghoon},
  year = {2024},
  month = may,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {35},
  number = {5},
  pages = {6613--6627},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2022.3211929},
  urldate = {2024-05-04},
  abstract = {Single-image 3-D reconstruction has long been a challenging problem. Recent deep learning approaches have been introduced to this 3-D area, but the ability to generate point clouds still remains limited due to inefficient and expensive 3-D representations, the dependency between the output and the number of model parameters, or the lack of a suitable computing operation. In this article, we present a novel deep-learning-based method to reconstruct a point cloud of an object from a single still image. The proposed method can be decomposed into two steps: feature fusion and deformation. The first step extracts both global and point-specific shape features from a 2-D object image, and then injects them into a randomly generated point cloud. In the second step, which is deformation, we introduce a new layer termed as GraphX that considers the interrelationship between points like common graph convolutions but operates on unordered sets. The framework can be applicable to realistic image data with background as we optionally learn a mask branch to segment objects from input images. To complement the quality of point clouds, we further propose an objective function to control the point uniformity. In addition, we introduce different variants of GraphX that cover from best performance to best memory budget. Moreover, the proposed model can generate an arbitrary-sized point cloud, which is the first deep method to do so. Extensive experiments demonstrate that we outperform the existing models and set a new height for different performance metrics in single-image 3-D reconstruction.},
  file = {/Users/jongyoo/Zotero/storage/ZU9TXM6D/Nguyen et al. - 2024 - Single-Image 3-D Reconstruction Rethinking Point .pdf}
}

@article{nguyenVideoFrameInterpolation2018,
  title = {Video Frame Interpolation by Plug-and-Play Deep Locally Linear Embedding},
  author = {Nguyen, Anh-Duc and Kim, Woojae and Kim, Jongyoo and Lee, Sanghoon},
  year = {2018},
  journal = {arXiv preprint arXiv:1807.01462},
  eprint = {1807.01462},
  archiveprefix = {arxiv},
  copyright = {All rights reserved}
}

@article{nguyenVideoFrameSynthesis2019,
  title = {Video {{Frame Synthesis Via Plug-and-Play Deep Locally Temporal Embedding}}},
  author = {Nguyen, Anh-Duc and Kim, Woojae and Kim, Jongyoo and Lin, Weisi and Lee, Sanghoon},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {179304--179319},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@inproceedings{oh3DVisualDiscomfort2015,
  title = {{{3D}} Visual Discomfort Predictor Based on Neural Activity Statistics},
  booktitle = {{{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Oh, Heeseok and Kim, Jongyoo and Lee, Sanghoon and Bovik, Alan C},
  year = {2015},
  pages = {3560--3564},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@article{ohBlindDeepS3D2017,
  title = {Blind Deep {{S3D}} Image Quality Evaluation via Local to Global Feature Aggregation},
  author = {Oh, Heeseok and Ahn, Sewoong and Kim, Jongyoo and Lee, Sanghoon},
  year = {2017},
  journal = {IEEE Transactions on Image Processing},
  volume = {26},
  number = {10},
  pages = {4923--4936},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@article{ohEnhancementVisualComfort2017,
  title = {Enhancement of Visual Comfort and Sense of Presence on Stereoscopic {{3D}} Images},
  author = {Oh, Heeseok and Kim, Jongyoo and Kim, Jinwoo and Kim, Taewan and Lee, Sanghoon and Bovik, Alan Conrad},
  year = {2017},
  journal = {IEEE Transactions on Image Processing},
  volume = {26},
  number = {8},
  pages = {3789--3801},
  publisher = {IEEE},
  copyright = {All rights reserved}
}
