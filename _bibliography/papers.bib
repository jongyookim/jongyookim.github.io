@article{gaoMPSNeRFGeneralizable3D2022,
  title = {{{MPS-NeRF}}: {{Generalizable 3D}} Human Rendering from Multiview Images},
  author = {Gao, Xiangjun and Yang, Jiaolong and Kim, Jongyoo and Peng, Sida and Liu, Zicheng and Tong, Xin},
  year = {2022},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@inproceedings{huangADNetLeveragingErrorbias2021,
  title = {{{ADNet}}: {{Leveraging}} Error-Bias towards Normal Direction in Face Alignment},
  booktitle = {{{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Huang, Yangyu and Yang, Hao and Li, Chong and Kim, Jongyoo and Wei, Fangyun},
  year = {2021},
  pages = {3080--3090},
  copyright = {All rights reserved}
}

@inproceedings{huangFreeEnricherEnrichingFace2023,
  title = {{{FreeEnricher}}: Enriching Face Landmarks without Additional Cost},
  booktitle = {{{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Huang, Yangyu and Chen, Xi and Kim, Jongyoo and Yang, Hao and Li, Chong and Yang, Jiaolong and Chen, Dong},
  year = {2023},
  volume = {37},
  pages = {962--970},
  copyright = {All rights reserved}
}

@article{kimBlindSharpnessPrediction2016,
  title = {Blind Sharpness Prediction for Ultrahigh-Definition Video Based on Human Visual Resolution},
  author = {Kim, Haksub and Kim, Jongyoo and Oh, Taegeun and Lee, Sanghoon},
  year = {2016},
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {27},
  number = {5},
  pages = {951--964},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@inproceedings{kimDeepBlindImage2017,
  title = {Deep Blind Image Quality Assessment by Employing {{FR-IQA}}},
  booktitle = {{{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Kim, Jongyoo and Lee, Sanghoon},
  year = {2017},
  pages = {3180--3184},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@inproceedings{kimDeepBlindImage2018,
  title = {Deep Blind Image Quality Assessment by Learning Sensitivity Map},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kim, Jongyoo and Kim, Woojae and Lee, Sanghoon},
  year = {2018},
  pages = {6727--6731},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@article{kimDeepCNNbasedBlind2018,
  title = {Deep {{CNN-based}} Blind Image Quality Predictor},
  author = {Kim, Jongyoo and Nguyen, Anh-Duc and Lee, Sanghoon},
  year = {2018},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {30},
  number = {1},
  pages = {11--24},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@article{kimDeepConvolutionalNeural2017,
  title = {Deep Convolutional Neural Models for Picture-Quality Prediction: {{Challenges}} and Solutions to Data-Driven Image Quality Assessment},
  author = {Kim, Jongyoo and Zeng, Hui and Ghadiyaram, Deepti and Lee, Sanghoon and Zhang, Lei and Bovik, Alan C},
  year = {2017},
  journal = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {6},
  pages = {130--141},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@inproceedings{kimDeepLearningHuman2017,
  title = {Deep Learning of Human Visual Sensitivity in Image Quality Assessment Framework},
  booktitle = {{{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kim, Jongyoo and Lee, Sanghoon},
  year = {2017},
  pages = {1676--1684},
  copyright = {All rights reserved}
}

@inproceedings{kimDeepVideoQuality2018,
  title = {Deep {{Video Quality Assessor}}: {{From Spatio-Temporal Visual Sensitivity}} to a {{Convolutional Neural Aggregation Network}}},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {Kim, Woojae and Kim, Jongyoo and Ahn, Sewoong and Kim, Jinwoo and Lee, Sanghoon},
  year = {2018},
  pages = {219--234},
  copyright = {All rights reserved}
}

@article{kimDiverseAdjustableVersatile2021,
  title = {Diverse and {{Adjustable Versatile Image Enhancer}}},
  author = {Kim, Woojae and Nguyen, Anh-Duc and Kim, Jinwoo and Kim, Jongyoo and Oh, Heeseok and Lee, Sanghoon},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {80883--80896},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@article{kimFullyDeepBlind2016,
  title = {Fully Deep Blind Image Quality Predictor},
  author = {Kim, Jongyoo and Lee, Sanghoon},
  year = {2016},
  journal = {IEEE Journal of selected topics in signal processing},
  volume = {11},
  number = {1},
  pages = {206--220},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@article{kimImplementationOmnidirectionalHuman2015,
  title = {Implementation of an Omnidirectional Human Motion Capture System Using Multiple Kinect Sensors},
  author = {Kim, Junghwan and Lee, Inwoong and Kim, Jongyoo and Lee, Sanghoon},
  year = {2015},
  journal = {IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences},
  volume = {98},
  number = {9},
  pages = {2004--2008},
  publisher = {{The Institute of Electronics, Information and Communication Engineers}},
  copyright = {All rights reserved}
}

@inproceedings{kimLearningHighFidelityFace2021,
  title = {Learning {{High-Fidelity Face Texture Completion Without Complete Face Texture}}},
  booktitle = {{{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Kim, Jongyoo and Yang, Jiaolong and Tong, Xin},
  year = {2021},
  pages = {13990--13999},
  copyright = {All rights reserved}
}

@article{kimMNETMusicDrivenPluralistic2023,
  title = {{{MNET}}++: {{Music-Driven Pluralistic Dancing Toward Multiple Dance Genre Synthesis}}},
  shorttitle = {{{MNET}}++},
  author = {Kim, Jinwoo and Kwon, Beom and Kim, Jongyoo and Lee, Sanghoon},
  year = {2023},
  month = dec,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {12},
  pages = {15036--15050},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2023.3312092},
  urldate = {2024-05-01},
  abstract = {Numerous task-specific variants of autoregressive networks have been developed for dance generation. Nonetheless, a severe limitation remains in that all existing algorithms can return repeated patterns for a given initial pose, which may be inferior. We examine and analyze several key challenges of previous works, and propose variations in both model architecture (namely MNET++) and training methods to address these. In particular, we devise the beat synchronizer and dance synthesizer. First, generated dance should be locally and globally consistent with given music beats, circumvent repetitive patterns, and look realistic. To achieve this, the beat synchronizer implicitly catches the rhythm enabling it to stay in sync with the music as it dances. Then, the dance synthesizer infers the dance motions in a seamless patch-by-patch manner conditioned by music. Second, to generate diverse dance lines, adversarial learning is performed by leveraging the transformer architecture. Furthermore, MNET++ learns a dance genre-aware latent representation that is scalable for multiple domains to provide fine-grained user control according to the dance genre. Compared with the state-of-the-art methods, our method synthesizes plausible and diverse outputs according to multiple dance genres as well as generates remarkable dance sequences qualitatively and quantitatively.},
  file = {files/4160/Kim et al. - 2023 - MNET++ Music-Driven Pluralistic Dancing Toward Mu.pdf}
}

@inproceedings{kimMultipleLevelFeaturebased2018,
  title = {Multiple Level Feature-Based Universal Blind Image Quality Assessment Model},
  booktitle = {{{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Kim, Jongyoo and Nguyen, Anh-Duc and Ahn, Sewoong and Luo, Chong and Lee, Sanghoon},
  year = {2018},
  pages = {291--295},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@inproceedings{kimNoreferencePerceptualSharpness2016,
  title = {No-Reference Perceptual Sharpness Assessment for Ultra-High-Definition Images},
  booktitle = {{{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Kim, Woojae and Kim, Haksub and Oh, Heeseok and Kim, Jongyoo and Lee, Sanghoon},
  year = {2016},
  pages = {86--90},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@article{kimPerceptualCrosstalkPrediction2016,
  title = {Perceptual Crosstalk Prediction on Autostereoscopic {{3D}} Display},
  author = {Kim, Taewan and Kim, Jongyoo and Kim, SeongYong and Cho, Sungho and Lee, Sanghoon},
  year = {2016},
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {27},
  number = {7},
  pages = {1450--1463},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@inproceedings{kimQualityAssessmentPerceptual2014,
  title = {Quality Assessment of Perceptual Crosstalk in Autostereoscopic Display},
  booktitle = {{{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Kim, Jongyoo and Kim, Taewan and Lee, Sanghoon},
  year = {2014},
  pages = {3484--3487},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@article{kimQualityAssessmentPerceptual2017,
  title = {Quality Assessment of Perceptual Crosstalk on Two-View Auto-Stereoscopic Displays},
  author = {Kim, Jongyoo and Kim, Taewan and Lee, Sanghoon and Bovik, Alan Conrad},
  year = {2017},
  journal = {IEEE Transactions on Image Processing},
  volume = {26},
  number = {10},
  pages = {4885--4899},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@inproceedings{kimVideoSharpnessPrediction2015,
  title = {Video Sharpness Prediction Based on Motion Blur Analysis},
  booktitle = {{{IEEE International Conference}} on {{Multimedia}} and {{Expo}} ({{ICME}})},
  author = {Kim, Jongyoo and Kim, Junghwan and Kim, Woojae and Lee, Jisoo and Lee, Sanghoon},
  year = {2015},
  pages = {1--6},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@article{kwonFrameworkImplementationImagebased2016,
  title = {Framework Implementation of Image-Based Indoor Localization System Using Parallel Distributed Computing},
  author = {Kwon, Beom and Jeon, Donghyun and Kim, Jongyoo and Kim, Junghwan and Kim, Doyoung and Song, Hyewon and Lee, Sanghoon},
  year = {2016},
  journal = {The Journal of Korean Institute of Communications and Information Sciences},
  volume = {41},
  number = {11},
  pages = {1490--1501},
  publisher = {{The Korean Institute of Commucations and Information Sciences}},
  copyright = {All rights reserved}
}

@inproceedings{kwonImplementationHumanAction2015,
  title = {Implementation of Human Action Recognition System Using Multiple {{Kinect}} Sensors},
  booktitle = {Advances in Multimedia Information {{Processing}}--{{PCM}} 2015},
  author = {Kwon, Beom and Kim, Doyoung and Kim, Junghwan and Lee, Inwoong and Kim, Jongyoo and Oh, Heeseok and Kim, Haksub and Lee, Sanghoon},
  year = {2015},
  pages = {334--343},
  publisher = {Springer International Publishing},
  copyright = {All rights reserved}
}

@article{leeDoubleReverseDiffusion2024,
  title = {Double Reverse Diffusion for Realistic Garment Reconstruction from Images},
  author = {Lee, Jeonghaeng and Nguyen, Duc and Kim, Jongyoo and Kang, Jiwoo and Lee, Sanghoon},
  year = {2024},
  month = jan,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {127},
  pages = {107404},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2023.107404},
  urldate = {2024-05-01},
  abstract = {Creating realistic digital 3D avatars has been getting more attention thanks to the introduction of new multimedia formats such as augmented and virtual reality. An important factor making avatars realistic is clothes. In this paper, we investigate a new method to reconstruct realistic garments from a set of images and body information. Early methods working on realistic images struggle to faithfully reconstruct the garment details. As deep learning is increasingly applied to geometric data which can conveniently represent garments, we devise a novel deep learning-based solution to the garment reconstruction problem. We offer a new perspective on the reconstruction problem and treat it as a reversion of the smoothing diffusion process. To achieve this goal, we propose to deform the smoothed human mesh into a clothed human via a Double Reverse Diffusion (DReD) process. For the first reverse diffusion, we introduce a novel operator called Graph Long Short-Term Memory (GraphLSTM) which recursively diffuses features to produce a deformed mesh by modeling the relationships between vertices. Then, the output mesh can be repeatedly upsampled and deformed by the above pipeline to obtain finer garment details, which can be seen as another reverse diffusion process. To obtain features for the reverse diffusion, we extract pixel-aligned features transferred from images and explore to incorporate the visibility of garments from the image viewpoints. Through detailed experiments on two public datasets, we demonstrate that DReD synthesizes more realistic wrinkled garments with lower errors and offers faster inference than previous methods.}
}

@article{leeIdentificationFrameworkPrintscan2017,
  title = {An Identification Framework for Print-Scan Books in a Large Database},
  author = {Lee, Sang-Hoon and Kim, Jongyoo and Lee, Sanghoon},
  year = {2017},
  journal = {Information sciences},
  volume = {396},
  pages = {33--54},
  publisher = {Elsevier},
  copyright = {All rights reserved}
}

@article{nguyenDeepVisualSaliency2018,
  title = {Deep Visual Saliency on Stereoscopic Images},
  author = {Nguyen, Anh-Duc and Kim, Jongyoo and Oh, Heeseok and Kim, Haksub and Lin, Weisi and Lee, Sanghoon},
  year = {2018},
  journal = {IEEE Transactions on Image Processing},
  volume = {28},
  number = {4},
  pages = {1939--1953},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@article{nguyenSingleimage3DReconstruction2022,
  title = {Single-Image 3-{{D}} Reconstruction: {{Rethinking}} Point Cloud Deformation},
  author = {Nguyen, Anh-Duc and Choi, Seonghwa and Kim, Woojae and Kim, Jongyoo and Oh, Heeseok and Kang, Jiwoo and Lee, Sanghoon},
  year = {2022},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@article{nguyenVideoFrameInterpolation2018,
  title = {Video Frame Interpolation by Plug-and-Play Deep Locally Linear Embedding},
  author = {Nguyen, Anh-Duc and Kim, Woojae and Kim, Jongyoo and Lee, Sanghoon},
  year = {2018},
  journal = {arXiv preprint arXiv:1807.01462},
  eprint = {1807.01462},
  archiveprefix = {arxiv},
  copyright = {All rights reserved}
}

@article{nguyenVideoFrameSynthesis2019,
  title = {Video {{Frame Synthesis Via Plug-and-Play Deep Locally Temporal Embedding}}},
  author = {Nguyen, Anh-Duc and Kim, Woojae and Kim, Jongyoo and Lin, Weisi and Lee, Sanghoon},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {179304--179319},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@inproceedings{oh3DVisualDiscomfort2015,
  title = {{{3D}} Visual Discomfort Predictor Based on Neural Activity Statistics},
  booktitle = {{{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Oh, Heeseok and Kim, Jongyoo and Lee, Sanghoon and Bovik, Alan C},
  year = {2015},
  pages = {3560--3564},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@article{ohBlindDeepS3D2017,
  title = {Blind Deep {{S3D}} Image Quality Evaluation via Local to Global Feature Aggregation},
  author = {Oh, Heeseok and Ahn, Sewoong and Kim, Jongyoo and Lee, Sanghoon},
  year = {2017},
  journal = {IEEE Transactions on Image Processing},
  volume = {26},
  number = {10},
  pages = {4923--4936},
  publisher = {IEEE},
  copyright = {All rights reserved}
}

@article{ohEnhancementVisualComfort2017,
  title = {Enhancement of Visual Comfort and Sense of Presence on Stereoscopic {{3D}} Images},
  author = {Oh, Heeseok and Kim, Jongyoo and Kim, Jinwoo and Kim, Taewan and Lee, Sanghoon and Bovik, Alan Conrad},
  year = {2017},
  journal = {IEEE Transactions on Image Processing},
  volume = {26},
  number = {8},
  pages = {3789--3801},
  publisher = {IEEE},
  copyright = {All rights reserved}
}
