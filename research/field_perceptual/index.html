<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Perceptual Image Processing | Multimedia AI Lab </title> <meta name="author" content="Multimedia AI Lab"> <meta name="description" content="Image &amp; video quality assessment, AR/VR QoE assessment"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon_128.png?ace3065a2426099ad17378689954309d"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jongyookim.github.io/research/field_perceptual/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand symbol"> <img src="/assets/img/icon_128.png" alt="MMAI Lab" width="30" height="30"> </div> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Multimedia AI</span> Lab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/research/">research <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/contact/">contact </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Perceptual Image Processing</h1> <p class="post-description">Image &amp; video quality assessment, AR/VR QoE assessment</p> </header> <article> <p>Digital images and videos are inevitably degraded in the process from content generation to consumption. The acquisition, transmission, scaling, format conversion, or compression of images and videos introduces various types of distortions, such as white noise, Gaussian blur, blocking artifacts, banding artifact and so on. Moreover, there are often multiple interacting distortions, which complicates the problem vastly. Since human observers are the ultimate receivers of digital images and videos, quality metrics should be designed from a human-oriented perspective. PI is one of the first pioneers applying deep neural networks to the domain of image quality assessment <a class="citation" href="#kimDeepConvolutionalNeural2017a">(Kim et al., 2017)</a>, <a class="citation" href="#kimDeepBlindImage2017">(Kim &amp; Lee, 2017)</a>, <a class="citation" href="#kimDeepBlindImage2018">(Kim et al., 2018)</a>, <a class="citation" href="#kimDeepCNNBasedBlind2019">(Kim et al., 2019)</a>, <a class="citation" href="#kimDeepLearningHuman2017">(Kim &amp; Lee, 2017)</a>, <a class="citation" href="#kimDeepVideoQuality2018">(Kim et al., 2018)</a>, <a class="citation" href="#kimFullyDeepBlind2017">(Kim &amp; Lee, 2017)</a>, <a class="citation" href="#kimMultipleLevelFeaturebased2018">(Kim et al., 2018)</a>, <a class="citation" href="#kimQualityAssessmentPerceptual2014">(Kim et al., 2014)</a>, <a class="citation" href="#kimQualityAssessmentPerceptual2017">(Kim et al., 2017)</a>, <a class="citation" href="#ohBlindDeepS3D2017">(Oh et al., 2017)</a>.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research/iqa_ssim-480.webp 480w,/assets/img/research/iqa_ssim-800.webp 800w,/assets/img/research/iqa_ssim-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/research/iqa_ssim.png" class="img-fluid" width="500" height="auto" title="SSIM" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Concept of classical image quality assessment. Even when various distortions exhibit the same mean squared errors, the perceived image quality can differ significantly among them. </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research/deep_iqa-480.webp 480w,/assets/img/research/deep_iqa-800.webp 800w,/assets/img/research/deep_iqa-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/research/deep_iqa.png" class="img-fluid" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Deep image quality assessment by employing pseudo ground-truth data. </div> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div id="kimDeepCNNBasedBlind2019" class="col-sm-10"> <div class="title">Deep CNN-Based Blind Image Quality Predictor</div> <div class="author"> <em>Jongyoo Kim</em>, Anh-Duc Nguyen, and Sanghoon Lee </div> <div class="periodical"> <em>IEEE Transactions on Neural Networks and Learning Systems</em>, Jan 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Image recognition based on convolutional neural networks (CNNs) has recently been shown to deliver the state-of-the-art performance in various areas of computer vision and image processing. Nevertheless, applying a deep CNN to no-reference image quality assessment (NR-IQA) remains a challenging task due to critical obstacles, i.e., the lack of a training database. In this paper, we propose a CNN-based NR-IQA framework that can effectively solve this problem. The proposed method—deep image quality assessor (DIQA)—separates the training of NR-IQA into two stages: 1) an objective distortion part and 2) a human visual system-related part. In the first stage, the CNN learns to predict the objective error map, and then the model learns to predict subjective score in the second stage. To complement the inaccuracy of the objective error map prediction on the homogeneous region, we also propose a reliability map. Two simple handcrafted features were additionally employed to further enhance the accuracy. In addition, we propose a way to visualize perceptual error maps to analyze what was learned by the deep CNN model. In the experiments, the DIQA yielded the state-of-the-art accuracy on the various databases.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div id="kimDeepBlindImage2018" class="col-sm-10"> <div class="title">Deep Blind Image Quality Assessment by Learning Sensitivity Map</div> <div class="author"> <em>Jongyoo Kim</em>, Woojae Kim, and Sanghoon Lee </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> , Jan 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="kimDeepVideoQuality2018" class="col-sm-10"> <div class="title">Deep Video Quality Assessor: From Spatio-Temporal Visual Sensitivity to a Convolutional Neural Aggregation Network</div> <div class="author"> Woojae Kim, <em>Jongyoo Kim</em>, Sewoong Ahn, Jinwoo Kim, and Sanghoon Lee </div> <div class="periodical"> <em>In European Conference on Computer Vision</em> , Jan 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="kimMultipleLevelFeaturebased2018" class="col-sm-10"> <div class="title">Multiple Level Feature-Based Universal Blind Image Quality Assessment Model</div> <div class="author"> <em>Jongyoo Kim</em>, Anh-Duc Nguyen, Sewoong Ahn, Chong Luo, and Sanghoon Lee </div> <div class="periodical"> <em>In IEEE International Conference on Image Processing (ICIP)</em> , Jan 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div id="kimDeepConvolutionalNeural2017a" class="col-sm-10"> <div class="title">Deep Convolutional Neural Models for Picture-Quality Prediction: Challenges and Solutions to Data-Driven Image Quality Assessment</div> <div class="author"> <em>Jongyoo Kim</em>, Hui Zeng, Deepti Ghadiyaram, Sanghoon Lee, Lei Zhang, and Alan C. Bovik </div> <div class="periodical"> <em>IEEE Signal Processing Magazine</em>, Nov 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Convolutional neural networks (CNNs) have been shown to deliver standout performance on a wide variety of visual information processing applications. However, this rapidly developing technology has only recently been applied with systematic energy to the problem of picture-quality prediction, primarily because of limitations imposed by a lack of adequate ground-truth human subjective data. This situation has begun to change with the development of promising data-gathering methods that are driving new approaches to deep-learning-based perceptual picture-quality prediction. Here, we assay progress in this rapidly evolving field, focusing, in particular, on new ways to collect large quantities of ground-truth data and on recent CNN-based picture-quality prediction models that deliver excellent results in a large, real-world, picture-quality database.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="kimDeepBlindImage2017" class="col-sm-10"> <div class="title">Deep Blind Image Quality Assessment by Employing FR-IQA</div> <div class="author"> <em>Jongyoo Kim</em>, and Sanghoon Lee </div> <div class="periodical"> <em>In IEEE International Conference on Image Processing (ICIP)</em> , Nov 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="kimDeepLearningHuman2017" class="col-sm-10"> <div class="title">Deep Learning of Human Visual Sensitivity in Image Quality Assessment Framework</div> <div class="author"> <em>Jongyoo Kim</em>, and Sanghoon Lee </div> <div class="periodical"> <em>In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> , Nov 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="kimFullyDeepBlind2017" class="col-sm-10"> <div class="title">Fully Deep Blind Image Quality Predictor</div> <div class="author"> <em>Jongyoo Kim</em>, and Sanghoon Lee </div> <div class="periodical"> <em>IEEE Journal of Selected Topics in Signal Processing</em>, Feb 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In general, owing to the benefits obtained from original information, full-reference image quality assessment (FR-IQA) achieves relatively higher prediction accuracy than no-reference image quality assessment (NR-IQA). By fully utilizing reference images, conventional FR-IQA methods have been investigated to produce objective scores that are close to subjective scores. In contrast, NR-IQA does not consider reference images; thus, its performance is inferior to that of FR-IQA. To alleviate this accuracy discrepancy between FR-IQA and NR-IQA methods, we propose a blind image evaluator based on a convolutional neural network (BIECON). To imitate FR-IQA behavior, we adopt the strong representation power of a deep convolutional neural network to generate a local quality map, similar to FR-IQA. To obtain the best results from the deep neural network, replacing hand-crafted features with automatically learned features is necessary. To apply the deep model to the NR-IQA framework, three critical problems must be resolved: 1) lack of training data; 2) absence of local ground truth targets; and 3) different purposes of feature learning. BIECON follows the FR-IQA behavior using the local quality maps as intermediate targets for conventional neural networks, which leads to NR-IQA prediction accuracy that is comparable with that of state-of-the-art FR-IQA methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="kimQualityAssessmentPerceptual2017" class="col-sm-10"> <div class="title">Quality Assessment of Perceptual Crosstalk on Two-View Auto-Stereoscopic Displays</div> <div class="author"> <em>Jongyoo Kim</em>, Taewan Kim, Sanghoon Lee, and Alan Conrad Bovik </div> <div class="periodical"> <em>IEEE Transactions on Image Processing</em>, Feb 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="ohBlindDeepS3D2017" class="col-sm-10"> <div class="title">Blind Deep S3D Image Quality Evaluation via Local to Global Feature Aggregation</div> <div class="author"> Heeseok Oh, Sewoong Ahn, <em>Jongyoo Kim</em>, and Sanghoon Lee </div> <div class="periodical"> <em>IEEE Transactions on Image Processing</em>, Feb 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2014</h2> <ol class="bibliography"><li> <div class="row"> <div id="kimQualityAssessmentPerceptual2014" class="col-sm-10"> <div class="title">Quality Assessment of Perceptual Crosstalk in Autostereoscopic Display</div> <div class="author"> <em>Jongyoo Kim</em>, Taewan Kim, and Sanghoon Lee </div> <div class="periodical"> <em>In IEEE International Conference on Image Processing (ICIP)</em> , Feb 2014 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Multimedia AI Lab. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>