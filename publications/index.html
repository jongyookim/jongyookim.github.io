<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Multimedia AI Lab </title> <meta name="author" content="Multimedia AI Lab"> <meta name="description" content="Journal and conference publications"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon_128.png?ace3065a2426099ad17378689954309d"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://mmai.yonsei.ac.kr/publications/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand symbol"> <img src="/assets/img/icon_128.png" alt="MMAI Lab" width="30" height="30"> </div> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Multimedia AI</span> Lab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">research </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/contact/">contact </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">Journal and conference publications</p> </header> <article> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div id="leeDoubleReverseDiffusion2024" class="col-sm-10"> <div class="title">Double Reverse Diffusion for Realistic Garment Reconstruction from Images</div> <div class="author"> Jeonghaeng Lee, Duc Nguyen, <em>Jongyoo Kim</em>, Jiwoo Kang, and Sanghoon Lee </div> <div class="periodical"> <em>Engineering Applications of Artificial Intelligence</em>, Jan 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Creating realistic digital 3D avatars has been getting more attention thanks to the introduction of new multimedia formats such as augmented and virtual reality. An important factor making avatars realistic is clothes. In this paper, we investigate a new method to reconstruct realistic garments from a set of images and body information. Early methods working on realistic images struggle to faithfully reconstruct the garment details. As deep learning is increasingly applied to geometric data which can conveniently represent garments, we devise a novel deep learning-based solution to the garment reconstruction problem. We offer a new perspective on the reconstruction problem and treat it as a reversion of the smoothing diffusion process. To achieve this goal, we propose to deform the smoothed human mesh into a clothed human via a Double Reverse Diffusion (DReD) process. For the first reverse diffusion, we introduce a novel operator called Graph Long Short-Term Memory (GraphLSTM) which recursively diffuses features to produce a deformed mesh by modeling the relationships between vertices. Then, the output mesh can be repeatedly upsampled and deformed by the above pipeline to obtain finer garment details, which can be seen as another reverse diffusion process. To obtain features for the reverse diffusion, we extract pixel-aligned features transferred from images and explore to incorporate the visibility of garments from the image viewpoints. Through detailed experiments on two public datasets, we demonstrate that DReD synthesizes more realistic wrinkled garments with lower errors and offers faster inference than previous methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="nguyenSingleImage3DReconstruction2024" class="col-sm-10"> <div class="title">Single-Image 3-D Reconstruction: Rethinking Point Cloud Deformation</div> <div class="author"> Anh-Duc Nguyen, Seonghwa Choi, Woojae Kim, <em>Jongyoo Kim</em>, Heeseok Oh, Jiwoo Kang, and Sanghoon Lee </div> <div class="periodical"> <em>IEEE Transactions on Neural Networks and Learning Systems</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Single-image 3-D reconstruction has long been a challenging problem. Recent deep learning approaches have been introduced to this 3-D area, but the ability to generate point clouds still remains limited due to inefficient and expensive 3-D representations, the dependency between the output and the number of model parameters, or the lack of a suitable computing operation. In this article, we present a novel deep-learning-based method to reconstruct a point cloud of an object from a single still image. The proposed method can be decomposed into two steps: feature fusion and deformation. The first step extracts both global and point-specific shape features from a 2-D object image, and then injects them into a randomly generated point cloud. In the second step, which is deformation, we introduce a new layer termed as GraphX that considers the interrelationship between points like common graph convolutions but operates on unordered sets. The framework can be applicable to realistic image data with background as we optionally learn a mask branch to segment objects from input images. To complement the quality of point clouds, we further propose an objective function to control the point uniformity. In addition, we introduce different variants of GraphX that cover from best performance to best memory budget. Moreover, the proposed model can generate an arbitrary-sized point cloud, which is the first deep method to do so. Extensive experiments demonstrate that we outperform the existing models and set a new height for different performance metrics in single-image 3-D reconstruction.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div id="huangFreeEnricherEnrichingFace2023" class="col-sm-10"> <div class="title">FreeEnricher: Enriching Face Landmarks without Additional Cost</div> <div class="author"> Yangyu Huang, Xi Chen, <em>Jongyoo Kim</em>, Hao Yang, Chong Li, Jiaolong Yang, and Dong Chen </div> <div class="periodical"> <em>In AAAI Conference on Artificial Intelligence</em> , May 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Recent years have witnessed significant growth of face alignment. Though dense facial landmark is highly demanded in various scenarios, e.g., cosmetic medicine and facial beautification, most works only consider sparse face alignment. To address this problem, we present a framework that can enrich landmark density by existing sparse landmark datasets, e.g., 300W with 68 points and WFLW with 98 points. Firstly, we observe that the local patches along each semantic contour are highly similar in appearance. Then, we propose a weakly-supervised idea of learning the refinement ability on original sparse landmarks and adapting this ability to enriched dense landmarks. Meanwhile, several operators are devised and organized together to implement the idea. Finally, the trained model is applied as a plug-and-play module to the existing face alignment networks. To evaluate our method, we manually label the dense landmarks on 300W testset. Our method yields state-of-the-art accuracy not only in newly-constructed dense 300W testset but also in the original sparse 300W and WFLW testsets without additional cost.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="kimMNETMusicDrivenPluralistic2023" class="col-sm-10"> <div class="title">MNET++: Music-Driven Pluralistic Dancing Toward Multiple Dance Genre Synthesis</div> <div class="author"> Jinwoo Kim, Beom Kwon, <em>Jongyoo Kim</em>, and Sanghoon Lee </div> <div class="periodical"> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, Dec 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Numerous task-specific variants of autoregressive networks have been developed for dance generation. Nonetheless, a severe limitation remains in that all existing algorithms can return repeated patterns for a given initial pose, which may be inferior. We examine and analyze several key challenges of previous works, and propose variations in both model architecture (namely MNET++) and training methods to address these. In particular, we devise the beat synchronizer and dance synthesizer. First, generated dance should be locally and globally consistent with given music beats, circumvent repetitive patterns, and look realistic. To achieve this, the beat synchronizer implicitly catches the rhythm enabling it to stay in sync with the music as it dances. Then, the dance synthesizer infers the dance motions in a seamless patch-by-patch manner conditioned by music. Second, to generate diverse dance lines, adversarial learning is performed by leveraging the transformer architecture. Furthermore, MNET++ learns a dance genre-aware latent representation that is scalable for multiple domains to provide fine-grained user control according to the dance genre. Compared with the state-of-the-art methods, our method synthesizes plausible and diverse outputs according to multiple dance genres as well as generates remarkable dance sequences qualitatively and quantitatively.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div id="gaoMPSNeRFGeneralizable3D2022a" class="col-sm-10"> <div class="title">MPS-NeRF: Generalizable 3D Human Rendering From Multiview Images</div> <div class="author"> Xiangjun Gao, Jiaolong Yang, <em>Jongyoo Kim</em>, Sida Peng, Zicheng Liu, and Xin Tong </div> <div class="periodical"> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, Dec 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>There has been rapid progress recently on 3D human rendering, including novel view synthesis and pose animation, based on the advances of neural radiance fields (NeRF). However, most existing methods focus on person-specific training and their training typically requires multi-view videos. This paper deals with a new challenging task – rendering novel views and novel poses for a person unseen in training, using only multiview still images as input without videos. For this task, we propose a simple yet surprisingly effective method to train a generalizable NeRF with multiview images as conditional input. The key ingredient is a dedicated representation combining a canonical NeRF and a volume deformation scheme. Using a canonical space enables our method to learn shared properties of human and easily generalize to different people. Volume deformation is used to connect the canonical space with input and target images and query image features for radiance and density prediction. We leverage the parametric 3D human model fitted on the input images to derive the deformation, which works quite well in practice when combined with our canonical NeRF. The experiments on both real and synthetic data with the novel view synthesis and pose animation tasks collectively demonstrate the efficacy of our method.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div id="huangADNetLeveragingErrorBias2021" class="col-sm-10"> <div class="title">ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment</div> <div class="author"> Yangyu Huang, Hao Yang, Chong Li, <em>Jongyoo Kim</em>, and Fangyun Wei </div> <div class="periodical"> <em>In IEEE/CVF International Conference on Computer Vision (ICCV)</em> , Oct 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The recent progress of CNN has dramatically improved face alignment performance. However, few works have paid attention to the error-bias with respect to error distribution of facial landmarks. In this paper, we investigate the error-bias issue in face alignment, where the distributions of landmark errors tend to spread along the tangent line to landmark curves. This error-bias is not trivial since it is closely connected to the ambiguous landmark labeling task. Inspired by this observation, we seek a way to leverage the error-bias property for better convergence of CNN model. To this end, we propose anisotropic direction loss (ADL) and anisotropic attention module (AAM) for coordinate and heatmap regression, respectively. ADL imposes strong binding force in normal direction for each landmark point on facial boundaries. On the other hand, AAM is an attention module which can get anisotropic attention mask focusing on the region of point and its local edge connected by adjacent points, it has a stronger response in tangent than in normal, which means relaxed constraints in the tangent. These two methods work in a complementary manner to learn both facial structures and texture details. Finally, we integrate them into an optimized end-to-end training pipeline named ADNet. Our ADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which demonstrates the effectiveness and robustness.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="kimDiverseAdjustableVersatile2021" class="col-sm-10"> <div class="title">Diverse and Adjustable Versatile Image Enhancer</div> <div class="author"> Woojae Kim, Anh-Duc Nguyen, Jinwoo Kim, <em>Jongyoo Kim</em>, Heeseok Oh, and Sanghoon Lee </div> <div class="periodical"> <em>IEEE Access</em>, Oct 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="kimLearningHighFidelityFace2021a" class="col-sm-10"> <div class="title">Learning High-Fidelity Face Texture Completion without Complete Face Texture</div> <div class="author"> <em>Jongyoo Kim</em>, Jiaolong Yang, and Xin Tong </div> <div class="periodical"> <em>In IEEE/CVF International Conference on Computer Vision (ICCV)</em> , Oct 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>For face texture completion, previous methods typically use some complete textures captured by multiview imaging systems or 3D scanners for supervised learning. This paper deals with a new challenging problem - learning to complete invisible texture in a single face image without using any complete texture. We simply leverage a large corpus of face images of different subjects (e. g., FFHQ) to train a texture completion model in an unsupervised manner. To achieve this, we propose DSD-GAN, a novel deep neural network based method that applies two discriminators in UV map space and image space. These two discriminators work in a complementary manner to learn both facial structures and texture details. We show that their combination is essential to obtain high-fidelity results. Despite the network never sees any complete facial appearance, it is able to generate compelling full textures from single images.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div id="kimDeepCNNBasedBlind2019" class="col-sm-10"> <div class="title">Deep CNN-Based Blind Image Quality Predictor</div> <div class="author"> <em>Jongyoo Kim</em>, Anh-Duc Nguyen, and Sanghoon Lee </div> <div class="periodical"> <em>IEEE Transactions on Neural Networks and Learning Systems</em>, Jan 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Image recognition based on convolutional neural networks (CNNs) has recently been shown to deliver the state-of-the-art performance in various areas of computer vision and image processing. Nevertheless, applying a deep CNN to no-reference image quality assessment (NR-IQA) remains a challenging task due to critical obstacles, i.e., the lack of a training database. In this paper, we propose a CNN-based NR-IQA framework that can effectively solve this problem. The proposed method—deep image quality assessor (DIQA)—separates the training of NR-IQA into two stages: 1) an objective distortion part and 2) a human visual system-related part. In the first stage, the CNN learns to predict the objective error map, and then the model learns to predict subjective score in the second stage. To complement the inaccuracy of the objective error map prediction on the homogeneous region, we also propose a reliability map. Two simple handcrafted features were additionally employed to further enhance the accuracy. In addition, we propose a way to visualize perceptual error maps to analyze what was learned by the deep CNN model. In the experiments, the DIQA yielded the state-of-the-art accuracy on the various databases.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="nguyenVideoFrameSynthesis2019" class="col-sm-10"> <div class="title">Video Frame Synthesis Via Plug-and-Play Deep Locally Temporal Embedding</div> <div class="author"> Anh-Duc Nguyen, Woojae Kim, <em>Jongyoo Kim</em>, Weisi Lin, and Sanghoon Lee </div> <div class="periodical"> <em>IEEE Access</em>, Jan 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div id="kimDeepBlindImage2018" class="col-sm-10"> <div class="title">Deep Blind Image Quality Assessment by Learning Sensitivity Map</div> <div class="author"> <em>Jongyoo Kim</em>, Woojae Kim, and Sanghoon Lee </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> , Jan 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="kimDeepVideoQuality2018" class="col-sm-10"> <div class="title">Deep Video Quality Assessor: From Spatio-Temporal Visual Sensitivity to a Convolutional Neural Aggregation Network</div> <div class="author"> Woojae Kim, <em>Jongyoo Kim</em>, Sewoong Ahn, Jinwoo Kim, and Sanghoon Lee </div> <div class="periodical"> <em>In European Conference on Computer Vision</em> , Jan 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="kimMultipleLevelFeaturebased2018" class="col-sm-10"> <div class="title">Multiple Level Feature-Based Universal Blind Image Quality Assessment Model</div> <div class="author"> <em>Jongyoo Kim</em>, Anh-Duc Nguyen, Sewoong Ahn, Chong Luo, and Sanghoon Lee </div> <div class="periodical"> <em>In IEEE International Conference on Image Processing (ICIP)</em> , Jan 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="nguyenDeepVisualSaliency2018" class="col-sm-10"> <div class="title">Deep Visual Saliency on Stereoscopic Images</div> <div class="author"> Anh-Duc Nguyen, <em>Jongyoo Kim</em>, Heeseok Oh, Haksub Kim, Weisi Lin, and Sanghoon Lee </div> <div class="periodical"> <em>IEEE Transactions on Image Processing</em>, Jan 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="nguyenVideoFrameInterpolation2018" class="col-sm-10"> <div class="title">Video Frame Interpolation by Plug-and-Play Deep Locally Linear Embedding</div> <div class="author"> Anh-Duc Nguyen, Woojae Kim, <em>Jongyoo Kim</em>, and Sanghoon Lee </div> <div class="periodical"> <em>arXiv preprint arXiv:1807.01462</em>, Jan 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div id="kimDeepBlindImage2017" class="col-sm-10"> <div class="title">Deep Blind Image Quality Assessment by Employing FR-IQA</div> <div class="author"> <em>Jongyoo Kim</em>, and Sanghoon Lee </div> <div class="periodical"> <em>In IEEE International Conference on Image Processing (ICIP)</em> , Jan 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="kimDeepConvolutionalNeural2017a" class="col-sm-10"> <div class="title">Deep Convolutional Neural Models for Picture-Quality Prediction: Challenges and Solutions to Data-Driven Image Quality Assessment</div> <div class="author"> <em>Jongyoo Kim</em>, Hui Zeng, Deepti Ghadiyaram, Sanghoon Lee, Lei Zhang, and Alan C. Bovik </div> <div class="periodical"> <em>IEEE Signal Processing Magazine</em>, Nov 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Convolutional neural networks (CNNs) have been shown to deliver standout performance on a wide variety of visual information processing applications. However, this rapidly developing technology has only recently been applied with systematic energy to the problem of picture-quality prediction, primarily because of limitations imposed by a lack of adequate ground-truth human subjective data. This situation has begun to change with the development of promising data-gathering methods that are driving new approaches to deep-learning-based perceptual picture-quality prediction. Here, we assay progress in this rapidly evolving field, focusing, in particular, on new ways to collect large quantities of ground-truth data and on recent CNN-based picture-quality prediction models that deliver excellent results in a large, real-world, picture-quality database.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="kimDeepLearningHuman2017" class="col-sm-10"> <div class="title">Deep Learning of Human Visual Sensitivity in Image Quality Assessment Framework</div> <div class="author"> <em>Jongyoo Kim</em>, and Sanghoon Lee </div> <div class="periodical"> <em>In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> , Nov 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="kimFullyDeepBlind2017" class="col-sm-10"> <div class="title">Fully Deep Blind Image Quality Predictor</div> <div class="author"> <em>Jongyoo Kim</em>, and Sanghoon Lee </div> <div class="periodical"> <em>IEEE Journal of Selected Topics in Signal Processing</em>, Feb 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In general, owing to the benefits obtained from original information, full-reference image quality assessment (FR-IQA) achieves relatively higher prediction accuracy than no-reference image quality assessment (NR-IQA). By fully utilizing reference images, conventional FR-IQA methods have been investigated to produce objective scores that are close to subjective scores. In contrast, NR-IQA does not consider reference images; thus, its performance is inferior to that of FR-IQA. To alleviate this accuracy discrepancy between FR-IQA and NR-IQA methods, we propose a blind image evaluator based on a convolutional neural network (BIECON). To imitate FR-IQA behavior, we adopt the strong representation power of a deep convolutional neural network to generate a local quality map, similar to FR-IQA. To obtain the best results from the deep neural network, replacing hand-crafted features with automatically learned features is necessary. To apply the deep model to the NR-IQA framework, three critical problems must be resolved: 1) lack of training data; 2) absence of local ground truth targets; and 3) different purposes of feature learning. BIECON follows the FR-IQA behavior using the local quality maps as intermediate targets for conventional neural networks, which leads to NR-IQA prediction accuracy that is comparable with that of state-of-the-art FR-IQA methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="kimQualityAssessmentPerceptual2017" class="col-sm-10"> <div class="title">Quality Assessment of Perceptual Crosstalk on Two-View Auto-Stereoscopic Displays</div> <div class="author"> <em>Jongyoo Kim</em>, Taewan Kim, Sanghoon Lee, and Alan Conrad Bovik </div> <div class="periodical"> <em>IEEE Transactions on Image Processing</em>, Feb 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="leeIdentificationFrameworkPrintscan2017" class="col-sm-10"> <div class="title">An Identification Framework for Print-Scan Books in a Large Database</div> <div class="author"> Sang-Hoon Lee, <em>Jongyoo Kim</em>, and Sanghoon Lee </div> <div class="periodical"> <em>Information sciences</em>, Feb 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="ohBlindDeepS3D2017" class="col-sm-10"> <div class="title">Blind Deep S3D Image Quality Evaluation via Local to Global Feature Aggregation</div> <div class="author"> Heeseok Oh, Sewoong Ahn, <em>Jongyoo Kim</em>, and Sanghoon Lee </div> <div class="periodical"> <em>IEEE Transactions on Image Processing</em>, Feb 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="ohEnhancementVisualComfort2017" class="col-sm-10"> <div class="title">Enhancement of Visual Comfort and Sense of Presence on Stereoscopic 3D Images</div> <div class="author"> Heeseok Oh, <em>Jongyoo Kim</em>, Jinwoo Kim, Taewan Kim, Sanghoon Lee, and Alan Conrad Bovik </div> <div class="periodical"> <em>IEEE Transactions on Image Processing</em>, Feb 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"> <li> <div class="row"> <div id="kimBlindSharpnessPrediction2016" class="col-sm-10"> <div class="title">Blind Sharpness Prediction for Ultrahigh-Definition Video Based on Human Visual Resolution</div> <div class="author"> Haksub Kim, <em>Jongyoo Kim</em>, Taegeun Oh, and Sanghoon Lee </div> <div class="periodical"> <em>IEEE Transactions on Circuits and Systems for Video Technology</em>, Feb 2016 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="kimNoreferencePerceptualSharpness2016" class="col-sm-10"> <div class="title">No-Reference Perceptual Sharpness Assessment for Ultra-High-Definition Images</div> <div class="author"> Woojae Kim, Haksub Kim, Heeseok Oh, <em>Jongyoo Kim</em>, and Sanghoon Lee </div> <div class="periodical"> <em>In IEEE International Conference on Image Processing (ICIP)</em> , Feb 2016 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="kimPerceptualCrosstalkPrediction2016" class="col-sm-10"> <div class="title">Perceptual Crosstalk Prediction on Autostereoscopic 3D Display</div> <div class="author"> Taewan Kim, <em>Jongyoo Kim</em>, SeongYong Kim, Sungho Cho, and Sanghoon Lee </div> <div class="periodical"> <em>IEEE Transactions on Circuits and Systems for Video Technology</em>, Feb 2016 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="kwonFrameworkImplementationImagebased2016" class="col-sm-10"> <div class="title">Framework Implementation of Image-Based Indoor Localization System Using Parallel Distributed Computing</div> <div class="author"> Beom Kwon, Donghyun Jeon, <em>Jongyoo Kim</em>, Junghwan Kim, Doyoung Kim, Hyewon Song, and Sanghoon Lee </div> <div class="periodical"> <em>The Journal of Korean Institute of Communications and Information Sciences</em>, Feb 2016 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"> <li> <div class="row"> <div id="kimImplementationOmnidirectionalHuman2015" class="col-sm-10"> <div class="title">Implementation of an Omnidirectional Human Motion Capture System Using Multiple Kinect Sensors</div> <div class="author"> Junghwan Kim, Inwoong Lee, <em>Jongyoo Kim</em>, and Sanghoon Lee </div> <div class="periodical"> <em>IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences</em>, Feb 2015 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="kimVideoSharpnessPrediction2015" class="col-sm-10"> <div class="title">Video Sharpness Prediction Based on Motion Blur Analysis</div> <div class="author"> <em>Jongyoo Kim</em>, Junghwan Kim, Woojae Kim, Jisoo Lee, and Sanghoon Lee </div> <div class="periodical"> <em>In IEEE International Conference on Multimedia and Expo (ICME)</em> , Feb 2015 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="kwonImplementationHumanAction2015" class="col-sm-10"> <div class="title">Implementation of Human Action Recognition System Using Multiple Kinect Sensors</div> <div class="author"> Beom Kwon, Doyoung Kim, Junghwan Kim, Inwoong Lee, <em>Jongyoo Kim</em>, Heeseok Oh, Haksub Kim, and Sanghoon Lee </div> <div class="periodical"> <em>In Advances in Multimedia Information Processing–PCM 2015</em> , Feb 2015 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="oh3DVisualDiscomfort2015" class="col-sm-10"> <div class="title">3D Visual Discomfort Predictor Based on Neural Activity Statistics</div> <div class="author"> Heeseok Oh, <em>Jongyoo Kim</em>, Sanghoon Lee, and Alan C Bovik </div> <div class="periodical"> <em>In IEEE International Conference on Image Processing (ICIP)</em> , Feb 2015 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2014</h2> <ol class="bibliography"><li> <div class="row"> <div id="kimQualityAssessmentPerceptual2014" class="col-sm-10"> <div class="title">Quality Assessment of Perceptual Crosstalk in Autostereoscopic Display</div> <div class="author"> <em>Jongyoo Kim</em>, Taewan Kim, and Sanghoon Lee </div> <div class="periodical"> <em>In IEEE International Conference on Image Processing (ICIP)</em> , Feb 2014 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Multimedia AI Lab. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>